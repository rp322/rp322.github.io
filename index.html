<!DOCTYPE HTML>
<html xml:lang="en" lang="en">
<head>
  <title>CSE455 | Final Project</title>
  <style media="all">
* { padding: 0; margin: 0; }
 body {
  margin: 0 auto 0 auto;
  padding: 0;
  max-width: 1200px;
  font-family: "Avenir", "Avenir Next", Helvetica Neue, Arial;
  font-size: 0.95em;
}
 a, a:visited { text-decoration: none; color: #7533f4; }
a:hover { text-decoration: underline; color: #f4b014; }
 h1, h2, h3, h4, h5 {
  color: #492a7c;
  background-color: inherit;
  font-weight: normal;
  padding: 0 0 5px 0;
  margin: 15px 0 0 0;
  border: none;
  clear: right;
}
h1 { font-size: 24pt; margin:  5px 0 10px 0; line-height: 28px; }
h2 { font-size: 14pt; margin: 30px 0 15px 0; letter-spacing: 0.01em; border-bottom: 1px solid #ccc;  line-height: 20px;}
h3 { font-size: 13pt; }
h4 { font-size: 12pt; }
h5 { font-size: 11pt; }
p { margin: 0 0 10px 0; }
 .content {
  margin: 0;
  padding: 15px 20px;
  background-color: #ffffff;
}
 .title, .title h1, .title a {
  color: #492a7c;
  font-size: 24pt;
  margin-bottom: 20px;
  margin-top: 5px;
}
 .footer {
  border-top: 1px solid #ccc;
  margin-top: 30px;
  padding-top: 4px;
  text-align: right;
  font-size: 12px;
}
.footer a {
  color: #21346B;
}
.footer a:hover {
  color: #ce3333;
}
  </style>
</head>
<body>
<div class="content">
   <section class="title">
    <a href="/">Exploring Increasing Diversity in Dropout Models</a>
  </section>

   <section>
    <p>
      <strong>Team Members</strong>: Preston Lee, Ryan Park
    </p>
  </section>

  <section>
    <h3>Overview</h3>
    <p>In machine learning, ensemble methods are techniques that are applied to algorithms that can provide more meaningful data points or even more accurate results. For example, algorithms such as logistic regression and na√Øve bayes may be used to predict a certain outcome, but we could potentially combine these algorithms together as some sort of aggregation. Or, we could train an algorithm on subsets of data and combine the results together. For the purpose of this project, we wanted to take a look into the dropout technique. Similar to the methods just mentioned, dropout is an ensemble technique to regularize neural networks. As there has been less research that has been done on the effect of model diversity within dropout, we wanted to do more research and experimentation on this for our final project. In our project, we will attempt to both explore the benefits/disadvantages of output diversity on a neural network outputs as well as find a scheme to enforce dropout diversity for better regularization in neural networks. We will conduct experiments on MNIST and CIFAR10 to compare diverse dropout vs. vanilla dropout.</p>
  </section>

  <section>
    <h3>Datasets</h3>
    <p>Our data set consist of MNIST and CIFAR10. MNIST is a database consisting of 60,000 training images and 10,000 testing images of hand written digits from 0-9. More information about this can be found on its wiki page: <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST Database</a>. Cifar-10 is a database consisting 60,000 32x32 images of 10 different classes including objects such as airplanes, cars, birds, etc. More information about this dataset can be found on its wiki page: <a href="https://en.wikipedia.org/wiki/CIFAR-10">CIFAR-10 Database</a></p>.
  </section>

  <section>
    <h3>Previous Work</h3>
    <p>To get more insight on ensemble and how to improve our machine learning models, we searched for online resources. We found the following article <a href="https://arxiv.org/abs/1908.11091">Deep Neural Network Ensembles against Deception: Ensemble Diversity, Accuracy and Robustness</a> to give a detailed breakdown to what ensemble is and how we can use techniques to strengthen our hypothesis and models. Furthermore, it also dives deep into math topics that explain how ensemble accuracy can be increased and what methods we can rely on to help us understand our data. For our code, we utilized pytorch and its neural network and adversarial testing libraries to help us with our project.</p>
  </section>

  <section>
    <h3>Our Approach</h3>
    <p>
      Our code is located on the following github page <a href="https://github.com/pfhgetty/dropout-diversity">Dropout Diversity</a></p>
  </section>
  
  <section>
    <h3>Results</h3>
    <p> We compiled the following graphs and data points:</p>
    <img src="images/AdversarialTestingCIFAR10.png" width="800" height="500">
    <img src="images/AdversarialTestingMNIST.png" width="800" height="500">
    <img src="images/CIFARTestAccuracy.png" width="800" height="500">
    <img src="images/MNISTTestAccuracy.png" width="800" height="500">
    
  </section>
  
  <section>
    <h3>Discussion</h3>
    <p>
      One technique that we were also thinking about working on was the stacking technique, which combines multiple different kinds of machine learning algorithms to create a polished model or analyze the similarities and differences to conclude which models and data we can rely on the most, as there is freedom and lots of variation when it comes to defining parameters in machine learning problems. 
    </p>
  </section>
  
  <section>
    <h3>Video</h3>
    <p>YOUTUBE</p>
  </section>

  <div class="footer">
    <a href="https://courses.cs.washington.edu/courses/cse442/20au/">CSE 455 Computer Vision</a>
    <a href="http://www.washington.edu">University of Washington</a>
  </div>
 </div>
</body>
</html> 
